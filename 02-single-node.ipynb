{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right\" src=\"img/saturn.png\" width=\"300\" />\n",
    "\n",
    "# Machine Learning on Big Data with Dask\n",
    "\n",
    "## Single-node workflow\n",
    "\n",
    "We'll first start off with a typical data preparation and machine learning workflow utilizing only the Jupyter Server.\n",
    "\n",
    "\n",
    "## Monitor resource utilization\n",
    "\n",
    "For this workshop it's important to monitor CPU and memory utilization when running various commands. It will help with understanding which operations are slow - and which ones run faster on a cluster!\n",
    "\n",
    "To monitor resource utilization of the Jupyter Server, open a new Terminal window inside Jupyter Lab and run `htop`. You can position the window to view the notebook and terminal on the same screen:\n",
    "\n",
    "![htop](img/htop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "This workshop will utilize [NYC taxi data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) for yellow taxi rides from the 2019 calendar year. The machine learning exercises involve predicting the \"tip fraction\" of each ride - how much a rider will tip the driver as a fraction of the charged fare amount.\n",
    "\n",
    "Let's operate with one month for now to explore the data and build out the machine learning code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "taxi = pd.read_csv(\n",
    "    'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-01.csv',\n",
    "    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "How many rows are in the `taxi` DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory usage is also an important consideration, as DataFrames often take more space in memory than on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_bytes = taxi.memory_usage(deep=True).sum()\n",
    "print(f\"Size (MB): {taxi_bytes / 1e6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis\n",
    "\n",
    "For this workshop, we will just look at column statistics. There are many more explorary analyses that can performed with `pandas` and data visualization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "np.round(taxi.describe().T, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "We are using stateless features, meaning the features values for a given observation don't depend on other observations. This is allows us to create features before performing any data splitting.\n",
    "\n",
    "Then, split data into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify feature and label column names\n",
    "raw_features = [\n",
    "    'tpep_pickup_datetime', \n",
    "    'passenger_count', \n",
    "    'tip_amount', \n",
    "    'fare_amount',\n",
    "]\n",
    "features = [\n",
    "    'pickup_weekday', \n",
    "    'pickup_weekofyear', \n",
    "    'pickup_hour', \n",
    "    'pickup_week_hour', \n",
    "    'pickup_minute', \n",
    "    'passenger_count',\n",
    "]\n",
    "label = 'tip_fraction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_df(taxi_df):\n",
    "    '''\n",
    "    Generate features from a raw taxi dataframe.\n",
    "    '''\n",
    "    df = taxi_df[taxi_df.fare_amount > 0][raw_features].copy()  # avoid divide-by-zero\n",
    "    df[label] = df.tip_amount / df.fare_amount\n",
    "     \n",
    "    df['pickup_weekday'] = df.tpep_pickup_datetime.dt.weekday\n",
    "    df['pickup_weekofyear'] = df.tpep_pickup_datetime.dt.isocalendar().week\n",
    "    df['pickup_hour'] = df.tpep_pickup_datetime.dt.hour\n",
    "    df['pickup_week_hour'] = (df.pickup_weekday * 24) + df.pickup_hour\n",
    "    df['pickup_minute'] = df.tpep_pickup_datetime.dt.minute\n",
    "    df = df[features + [label]].astype(float).fillna(-1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_feat = prep_df(taxi)\n",
    "taxi_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    taxi_feat[features], \n",
    "    taxi_feat[label], \n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "\n",
    "We'll train a linear model to predict `tip_fraction`. We define a `Pipeline` to encompass both feature scaling and model training. This will be useful later when performing a grid search.\n",
    "\n",
    "Evaluate the model against the test set using RMSE. We'll also save out the model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', ElasticNet(normalize=False, max_iter=100, l1_ratio=0)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fitted = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds = fitted.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, preds, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "\n",
    "with open('/tmp/model.pkl', 'wb') as f:\n",
    "    cloudpickle.dump(fitted, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooray!\n",
    "\n",
    "We trained a terrible model. But that's okay. The point of this workshop is to scale our work, not make the model perfect!\n",
    "\n",
    "# Let's step things up\n",
    "\n",
    "We were able to train a model on a sample of the taxi data (single month from 2019). In many model building settings more data would be required. Follow below to see where the single-node environment starts to encounter challenges - the rest of the workshop will cover how Dask solves these problems!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process large dataset\n",
    "\n",
    "Let's look at the size of the files on disk using `s3fs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "files = s3.glob('s3://nyc-tlc/trip data/yellow_tripdata_2019-*.csv')\n",
    "total_size = 0\n",
    "for f in files:\n",
    "    size = s3.du(f)\n",
    "    total_size += size\n",
    "    \n",
    "    print(f\"{f}, Size: {round(size / 1e6, 2)} MB\")\n",
    "print()\n",
    "print(f\"Total size: {round(total_size / 1e9, 2)} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Files end up taking more space when loaded into memory, but let's see if this will fit on our Jupyter Server. We can loop through the files and concatenate them into one DataFrame. Watch memory utilization as the loop runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file):\n",
    "    df = pd.read_csv(\n",
    "        s3.open(file, mode='rb'),\n",
    "        parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfs = []\n",
    "for f in files:\n",
    "    print(f)\n",
    "    df = load_csv(f)\n",
    "    print(f'{len(df)} rows, {df.memory_usage(deep=True).sum() / 1e6} MB')\n",
    "    dfs.append(df)\n",
    "taxi_big = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![explosion](https://media4.giphy.com/media/13d2jHlSlxklVe/giphy.gif)\n",
    "\n",
    "Oh no! Looks like we have enough memory to load the CSV files individually, but not to concatenate them together into one DataFrame.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with large dataset\n",
    "\n",
    "We are stuck here, because we need to be able to load the full dataset into memory to train with it.\n",
    "\n",
    "Think about all the data we're missing out on. All those observations, all those models that will never get a chance!\n",
    "\n",
    "<img src=\"https://media0.giphy.com/media/k61nOBRRBMxva/giphy.gif\" width=\"400\" alt=\"crying\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There must be a better way!\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/latest/_images/dask_horizontal_no_pad.svg\" width=\"300\" alt=\"dask\" />\n",
    "\n",
    "With Dask, we can scale out to a cluster to address these problems. \n",
    "\n",
    "Move on to [03-dask-basics.ipynb](03-dask-basics.ipynb) to get started!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
