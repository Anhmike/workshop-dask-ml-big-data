{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right\" src=\"img/saturn.png\" width=\"300\" />\n",
    "\n",
    "# Machine Learning on Big Data with Dask\n",
    "\n",
    "## Processing Large datasets\n",
    "\n",
    "This notebook shows how to process large datasets with Dask in preparation for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dask cluster\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Create a `cluster` and `client` object to connect to our Dask cluster. Check out [03-dask-basics.ipynb](03-dask-basics.ipynb) if you need some help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = SaturnCluster(\n",
    "    <<< FILL IN >>>\n",
    ")\n",
    "client = <<< FILL IN >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = SaturnCluster(\n",
    "    scheduler_size='medium',\n",
    "    worker_size='xlarge',\n",
    "    n_workers=5,\n",
    "    nthreads=4,\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.wait_for_workers(5)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process large dataset\n",
    "\n",
    "## Load data\n",
    "\n",
    "Our large dataset for this notebook will be NYC taxi data from all of 2019 (in [02-single-node.ipynb](02-single-node.ipynb) we just used one month of 2019). Rather than load the data with pandas' `pd.read_csv`, we will use Dask's [`dd.read_csv` method](https://docs.dask.org/en/latest/dataframe-create.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import wait\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dd.read_csv` accepts glob syntax for loading in multiple files. This way, we don't have to write a for loop and concatenate DataFrames like we tried with pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_2019 = 's3://nyc-tlc/trip data/yellow_tripdata_2019-*.csv'\n",
    "s3.glob(files_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We need to pass a couple of extra arguments to `dd.read_csv`:\n",
    "- `storage_options=...`: this tells Dask to use anonymous S3 access (we did this with `s3.open` for pandas)\n",
    "- `assuming_missing=True`: this tells Dask to read all numeric columns as floats. Dask sometimes needs type information up-front to be able to parallelize tasks effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "taxi = dd.read_csv(\n",
    "    files_2019,\n",
    "    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'],\n",
    "    storage_options={'anon': True},\n",
    "    assume_missing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that previewing the `taxi` object doesn't print out the contents of the DataFrame, like with pandas. This is because Dask has not yet loaded any data. It does tell us the number of partitions (i.e. little pandas DataFrames) the big Dask DataFrame has. \n",
    "\n",
    "### Exercise\n",
    "\n",
    "Compute the number of rows in the `taxi` DataFrame (hint: think pandas API!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<< FILL IN >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "len(taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "You may have tried `taxi.shape` and gotten some unfamiliar output. This because of Dask's lazy evaluation - Dask doesn't perform any operations until asked to. `len` is a special case that triggers computation. If we want to get the row count of out `taxi.shape`, we need to run `.compute()` on the delayed object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.shape[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_bytes = taxi.memory_usage(deep=True).sum()\n",
    "taxi_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the above cell completed immediately (but we don't have the result). A comparable pandas call would take a few seconds to compute the memory usage. You guessed it - lazy evaluation! \n",
    "\n",
    "### Exercise\n",
    "\n",
    "Trigger computation on `taxi_bytes` to get the actual size of the Dask DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"Size (MB): {<<< FILL IN >>>}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"Size (MB): {taxi_bytes.compute() / 1e6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "`.compute()` returns results immediately. Be careful though, because if you run `taxi.compute()` Dask will give you the entire big DataFrame as a pandas object (this will certainly blow up the kernel!).\n",
    "\n",
    "It it useful in many cases to trigger computation on objects even if you don't want to pull them down to the Jupyter Server. In this case we use `.persist()`, which triggers all computations performed on the DataFrame and holds the results in memory across the _cluster_. More discussion about `.compute()` vs. `.persist()` is in [03-dask-basics.ipynb](03-dask-basics.ipynb). Persisting becomes useful when we perform later machine learning operations, as we don't want Dask to be re-parsing CSV files in each iteration of model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = taxi.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the above cell completed immediately but the Dask Dashboard is still doing work. We can use the `wait()` function to block our notebook until the `taxi` DataFrame is fully done persisting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = wait(taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will run much faster than before! This is because the DataFrame is loaded up into memory across the cluster, and Dask does not need to download and parse the CSV files again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "taxi_bytes = taxi.memory_usage(deep=True).sum()\n",
    "print(f\"Size (MB): {taxi_bytes.compute() / 1e6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messy data - `dask.delayed`\n",
    "\n",
    "> Advanced topic: This section is optional. You may continue to the **Exploratory Analysis** section if you want to skip this.\n",
    "\n",
    "Data files aren't always provided in a clean tabular form thats readable with a `read_*` method from pandas or Dask. With [`dask.delayed` functions](https://docs.dask.org/en/latest/delayed.html), we can write a function that processes a single chunk of raw data and then tell Dask to collect these into a Dask DataFrame. We'll illustrate that now with the CSV files, but its always better to use a `dd.read_*` method if your data supports it. `dask.delayed` is very flexible and powerful - chances are you will use it for some of your workloads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def make_data(i):\n",
    "    return pd.DataFrame([(i,), ], columns=['foo'])\n",
    "\n",
    "dfs = []\n",
    "for i in range(10):\n",
    "    df = make_data(i)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_dataframe = dd.from_delayed(dfs)\n",
    "dask_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we can only call `.compute()` here because we know the dataframe is small!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_dataframe.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Define a delayed function, `load_csv` that will return a pandas DataFrame for a given NYC taxi file path. (Hint: a similar function was created in [02-single-node.ipynb](02-single-node.ipynb)). Test your function with this path:\n",
    "\n",
    "```https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-01.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def load_csv(path):\n",
    "    <<< FILL IN >>>\n",
    "    \n",
    "df = <<< FILL IN >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def load_csv(path):\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "    )\n",
    "    return df\n",
    "    \n",
    "df = load_csv('https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-01.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "Now we can call this for the 2019 files and create a Dask DataFrame with `dd.from_delayed()`. `from_delayed()` expects a list of delayed objects that return a pandas dataframe. It will then trigger the loading across the cluster and create a distributed dataframe object that wraps the pandas dataframes.\n",
    "\n",
    "Notice that this for loop looks very similar to what blew up our kernel in [02-single-node.ipynb](02-single-node.ipynb). Because of Dask's lazy evaluation none of these functions actually pull data until we perform operations with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for f in s3.glob(files_2019):\n",
    "    df = load_csv(f's3://{f}')\n",
    "    dfs.append(df)\n",
    "taxi_delayed = dd.from_delayed(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_delayed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just an example of how you would use `dask.delayed` to pull data from anywhere into a distributed dataframe. Because our NYC taxi data is already in CSV format, we can use the native `dask.dataframe.read_csv` method, which is also more efficient than a custom delayed workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis\n",
    "\n",
    "We'll go back to using the `taxi` Dask DataFrame we loaded with `dd.read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "taxi_describe = taxi.describe().compute().T\n",
    "np.round(taxi_describe, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Do some of your own exploratory analysis with the `taxi` dataframe. Try things that you would normally do with pandas and see how the Dask dataframe reacts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<<< FILL IN >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# There's no right answer here! Hopefully you learned something about Dask :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify feature and label column names\n",
    "raw_features = [\n",
    "    'tpep_pickup_datetime', \n",
    "    'passenger_count', \n",
    "    'tip_amount', \n",
    "    'fare_amount',\n",
    "]\n",
    "features = [\n",
    "    'pickup_weekday', \n",
    "    'pickup_weekofyear', \n",
    "    'pickup_hour', \n",
    "    'pickup_week_hour', \n",
    "    'pickup_minute', \n",
    "    'passenger_count',\n",
    "]\n",
    "label = 'tip_fraction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Copy over the `prep_df()` function from [02-single-node.ipynb](02-single-node.ipynb). Then:\n",
    "- Apply the function to `taxi` and save that in a new dataframe called `taxi_feat`. \n",
    "- Preview the results\n",
    "\n",
    "Are any changes needed to `prep_df()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_df(taxi_df):\n",
    "    <<< FILL IN >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_feat = <<< FILL IN >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def prep_df(taxi_df):\n",
    "    '''\n",
    "    Generate features from a raw taxi dataframe.\n",
    "    '''\n",
    "    df = taxi_df[taxi_df.fare_amount > 0][raw_features].copy()  # avoid divide-by-zero\n",
    "    df[label] = df.tip_amount / df.fare_amount\n",
    "     \n",
    "    df['pickup_weekday'] = df.tpep_pickup_datetime.dt.weekday\n",
    "    df['pickup_weekofyear'] = df.tpep_pickup_datetime.dt.isocalendar().week\n",
    "    df['pickup_hour'] = df.tpep_pickup_datetime.dt.hour\n",
    "    df['pickup_week_hour'] = (df.pickup_weekday * 24) + df.pickup_hour\n",
    "    df['pickup_minute'] = df.tpep_pickup_datetime.dt.minute\n",
    "    df = df[features + [label]].astype(float).fillna(-1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "taxi_feat = prep_df(taxi)\n",
    "taxi_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this feature engineering code is _exactly_ the same as what we did in [02-single-node.ipynb](02-single-node.ipynb). Dask' DataFrame API matches pandas' API in many places. Check out the [Dask DataFrame docs](https://docs.dask.org/en/latest/dataframe.html#dask-dataframe-copies-the-pandas-api) for more information on what is and is not supported from the pandas API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
