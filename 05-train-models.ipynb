{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right\" src=\"img/saturn.png\" width=\"300\" />\n",
    "\n",
    "# Machine Learning on Big Data with Dask\n",
    "\n",
    "## Train models!\n",
    "\n",
    "Now that we can process big data as illustrated in [04-large-dataset.ipynb](04-large-dataset.ipynb), let's train some models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize cluster and load dataset\n",
    "\n",
    "The [05-setup.py](05-setup.py) contains the same data loading code from [04-large-dataset.ipynb](04-large-dataset.ipynb). After running that, we have the following objects available:\n",
    "\n",
    "- `client`: Dask client\n",
    "- `taxi_feat`: Taxi dataframe with engineering features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Persist the `taxi_feat` dataframe to make sure subsequent operations are fast. Block until it's completely finished persisting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "<<< FILL IN >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "taxi_feat = taxi_feat.persist()\n",
    "_ = wait(taxi_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with large dataset\n",
    "\n",
    "First, we need to split our `taxi_feat` DataFrame into train/test sets.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Use the [`dask_ml.model_selection.train_test_split` function](https://ml.dask.org/modules/generated/dask_ml.model_selection.train_test_split.html) to split into train and test sets. Use 30% of the data for the test set.\n",
    "\n",
    "> Hint: the `dask_ml` function works the same as the `sklearn` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = <<< FILL IN >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    taxi_feat[features], \n",
    "    taxi_feat[label], \n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Dask ML models\n",
    "\n",
    "The dask-ml package has parallel implementations of machine learning algorithms that do not have parallel implementations in scikit-learn or other packages. These currently cover linear models and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.metrics import mean_squared_error\n",
    "\n",
    "lr = Pipeline(steps=[\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', LinearRegression(penalty='l2', max_iter=100)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask-ml's `LinearRegression` model requires arrays as inputs rather than dataframes, so we convert them to arrays. Similar to the relationship between a pandas dataframe and a Dask dataframe, Dask arrays are collections of numpy arrays that conform to a similar API.\n",
    "\n",
    "With pandas dataframes, we could call `.values` to get a numpy array. Dask needs more information about chunk sizes of the underlying numpy arrays, so we need to do `.to_dask_array(lengths=True)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_arr = X_train.to_dask_array(lengths=True)\n",
    "X_test_arr = X_test.to_dask_array(lengths=True)\n",
    "y_train_arr = y_train.to_dask_array(lengths=True)\n",
    "y_test_arr = y_test.to_dask_array(lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to Dask's lazy evaluation, these dataframes have not been computed yet. To ensure the rest of our ML code runs quickly, lets kick off computation on the cluster by calling `persist()` on the arrays. Note that there is a `dask.persist` function that accepts multiple objects rather than calling `.persist()` individually. This is helpful for objects that share upstream tasks - Dask will avoid re-computing the shared tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train_arr, X_test_arr, y_train_arr, y_test_arr = dask.persist(\n",
    "    X_train_arr, X_test_arr, y_train_arr, y_test_arr,\n",
    ")\n",
    "_ = wait(X_train_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Train the `lr` model with `X_train_arr` and `y_train_arr` as input.\n",
    "\n",
    "> Note: this will take a few minutes because we are training with a pretty large dataset. You can scale up your cluster if you want it to execute faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lr_fitted = <<< FILL IN >>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lr_fitted = lr.fit(\n",
    "    X_train_arr,\n",
    "    y_train_arr,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = lr_fitted.predict(X_test_arr)\n",
    "mean_squared_error(y_test_arr, lr_preds, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "XGBoost supports distributed model training across Dask clusters in the `xgboost.dask` module. For a deeper dive into how this works, check out our tutorial on [XGBoost Training with Dask](https://www.saturncloud.io/docs/tutorials/xgboost/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.dask import DaskXGBRegressor\n",
    "\n",
    "xgb = DaskXGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    tree_method='hist',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    n_estimators=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgb_fitted = xgb.fit(\n",
    "    X_train_arr,\n",
    "    y_train_arr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_preds = xgb_fitted.predict(X_test_arr)\n",
    "mean_squared_error(y_test_arr, xgb_preds, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost also supports GPU acceleration across one or more GPUs in a Dask cluster. Stay tuned for a future workshop on all things Dask+XGBoost!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
